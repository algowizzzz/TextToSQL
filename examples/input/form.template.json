{
  "_comment": "BA INPUT FORM - Single source of truth for all configuration",
  "_instructions": [
    "1. Set mode to 'parquet' or 'api'",
    "2. Configure tables with columns and data sources",
    "3. Add vocabulary (synonyms) as needed",
    "4. Optionally adjust limits, prompts, and model",
    "5. Run: python json_sql_copilot.py --form input/form.json --q 'your query'"
  ],
  
  "mode": "parquet",
  
  "tables": {
    "ccr_limits": {
      "description": "CCR exposure vs limits by counterparty",
      "columns": [
        "adaptiv_code", "customer_name", "sector", "rating", "country", "region",
        "portfolio", "booking_location", "currency", "as_of_date",
        "exposure_epe", "exposure_pfe", "exposure_ead", "exposure_var", "exposure_stress",
        "limit_ccr", "limit_type", "limit_buffer", "limit_utilization_pct", "risk_owner"
      ],
      "source": {
        "_comment_parquet_mode": "When mode='parquet', provide file_path to .parquet file (fast, handles 100M+ rows)",
        "file_path": "input/data/ccr_limits.parquet",
        
        "_comment_api_mode": "When mode='api', provide these fields instead",
        "_api_example": {
          "url": "https://api.example.com/ccr/limits",
          "headers": {
            "Authorization": "Bearer ${CCR_API_TOKEN}"
          },
          "format": "csv_rows_in_json",
          "_format_options": ["array_of_objects", "csv_rows_in_json", "csv_url"],
          "columns_key": "columns",
          "row_key": "rows",
          "field_key": null,
          "delimiter": ","
        }
      }
    },
    "trades": {
      "description": "Trade-level data for customers with limits",
      "columns": [
        "trade_id", "adaptiv_code", "product", "notional", "mtm", "asset_class",
        "desk", "book", "trade_date", "maturity_date", "counterparty", "netting_set",
        "csa_flag", "collateralized", "risk_factor", "delta", "gamma", "vega", "pnl", "currency"
      ],
      "source": {
        "file_path": "input/data/trades.parquet"
      }
    }
  },
  
  "vocabulary": {
    "_comment": "Map natural language terms to database columns",
    "adaptiv code": "adaptiv_code",
    "utilization": "limit_utilization_pct",
    "limit utilization": "limit_utilization_pct",
    "counterparty code": "adaptiv_code",
    "breach": "limit_utilization_pct > 100",
    "headroom": "limit_ccr - exposure_pfe"
  },
  
  "limits": {
    "_comment": "SQL execution guardrails",
    "default_limit": 200,
    "hard_max_rows": 1000,
    "allow_non_select": false,
    "timeout_seconds": 30
  },
  
  "prompts": {
    "_comment": "LLM prompt templates (edit with caution)",
    "dialect_hint": "DuckDB SQL",
    "system": "You are a senior data analyst who writes safe, efficient SQL for {dialect_hint}. Use ONLY the provided tables/columns. Return a single runnable SQL statement. No commentary.",
    "user_template": "SCHEMA:\n{schema_text}\n\nVOCABULARY:\n{vocabulary}\n\nTASK:\nWrite a single {dialect_hint} SQL statement for:\n\"{user_request}\"\n\nRULES:\n- Use ONLY tables/columns in SCHEMA.\n- Join tables via adaptiv_code when needed.\n- If sampling implied, apply LIMIT {default_limit}.\n- SELECT/CTEs only; return SQL only."
  },
  
  "model": {
    "_comment": "LLM configuration (requires OPENAI_API_KEY in .env)",
    "provider": "openai",
    "name": "gpt-4o-mini",
    "temperature": 0.0
  },
  
  "commentary": {
    "_comment": "Auto-generate natural language insights about query results (requires --use-llm or --with-commentary)",
    "enabled": true,
    "max_rows_in_prompt": 20,
    "system_prompt": "You are a senior risk analyst providing concise insights on CCR exposure and trading data. Be specific with numbers, highlight key risks, and keep responses to 3-5 sentences.",
    "user_template": "USER ASKED: \"{user_request}\"\n\nQUERY RESULTS ({row_count} rows):\nColumns: {columns}\n\nSample Data (first {sample_size} rows):\n{data_preview}\n\nProvide a brief, insightful commentary (3-5 sentences) that DIRECTLY ANSWERS the user's question using this data. Focus on what the user specifically asked about, highlighting key risks, exposures, or notable patterns relevant to their request."
  },
  
  "optimization": {
    "_comment": "Performance optimizations for large datasets (75M+ rows)",
    "two_stage_optimizer": true,
    "_explanation": "Two-stage: Extract WHERE filters first (Stage 1), then generate optimized SQL (Stage 2). Reduces LLM costs 30-50%, improves query performance with predicate pushdown."
  },
  
  "rule_based_queries": {
    "_comment": "Pre-built queries that work without LLM",
    "top breaches headroom": "SELECT l.adaptiv_code, l.customer_name, l.limit_utilization_pct, (l.limit_ccr - l.exposure_pfe) AS headroom FROM ccr_limits l WHERE l.limit_utilization_pct > 100 ORDER BY l.limit_utilization_pct DESC"
  }
}

