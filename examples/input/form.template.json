{
  "_comment": "BA INPUT FORM - Single source of truth for all configuration",
  "_instructions": [
    "1. Set mode to 'parquet' or 'api'",
    "2. Configure tables with columns and data sources",
    "3. Add vocabulary (synonyms) as needed",
    "4. Optionally adjust limits, prompts, and model",
    "5. Run: python json_sql_copilot.py --form input/form.json --q 'your query'"
  ],
  
  "mode": "parquet",
  
  "schema": {
    "_comment": "Entity-Relationship documentation for LLM understanding",
    "relationship": "ccr_limits (1) ←→ (N) trades via adaptiv_code",
    "tables": {
      "ccr_limits": {
        "entity": "Counterparty credit exposure aggregates (one row per counterparty per date)",
        "columns": {
          "adaptiv_code": "Primary key: Unique counterparty identifier (FK to trades)",
          "customer_name": "Counterparty legal name",
          "exposure_epe": "Expected Positive Exposure (average future exposure)",
          "exposure_pfe": "Potential Future Exposure (worst-case at confidence level)",
          "exposure_ead": "Exposure at Default (regulatory capital measure)",
          "limit_ccr": "Approved credit limit for counterparty",
          "limit_buffer": "Headroom: limit_ccr - exposure_pfe",
          "limit_utilization_pct": "Utilization percentage: (exposure_pfe / limit_ccr) * 100. >100 = BREACH",
          "currency": "Reporting currency (USD/CAD)",
          "booking_location": "Trading location (TOR/NYC/LDN/HKG/SYD)",
          "portfolio": "Risk portfolio (FX-Derivs/Rates/Commodities/Credit/Equities)",
          "risk_owner": "Responsible desk",
          "as_of_date": "Business date (VARCHAR, use CAST for comparisons)"
        }
      },
      "trades": {
        "entity": "Individual trade positions (many rows per counterparty)",
        "columns": {
          "trade_id": "Primary key: Unique trade identifier",
          "adaptiv_code": "Foreign key: Links to ccr_limits.adaptiv_code",
          "product": "Trade product type (FX FWD/IRS/CDS/etc)",
          "notional": "Trade notional amount",
          "mtm": "Mark-to-Market value (current P&L)",
          "asset_class": "FX/Rates/Commodities/Credit/Equities",
          "desk": "Trading desk",
          "book": "Trading book code",
          "trade_date": "Trade execution date",
          "maturity_date": "Trade maturity date",
          "counterparty": "Counterparty name (same as customer_name in ccr_limits)",
          "netting_set": "ISDA netting agreement ID",
          "csa_flag": "Credit Support Annex exists (boolean)",
          "collateralized": "Trade is collateralized (boolean)",
          "risk_factor": "Primary risk factor",
          "delta": "Price sensitivity",
          "gamma": "Delta sensitivity (convexity)",
          "vega": "Volatility sensitivity",
          "pnl": "Profit and loss",
          "currency": "Trade currency",
          "failed_trade": "Exposure calc failed, exposure = notional (overstated)",
          "as_of_date": "Business date (VARCHAR, use CAST for comparisons)"
        }
      }
    },
    "constraints": {
      "join": "ccr_limits.adaptiv_code = trades.adaptiv_code",
      "date_filter": "CAST(as_of_date AS DATE) for comparisons (stored as VARCHAR)",
      "column_ownership": {
        "limit_utilization_pct": "ONLY in ccr_limits",
        "exposure_*": "ONLY in ccr_limits",
        "limit_*": "ONLY in ccr_limits",
        "trade_id": "ONLY in trades",
        "product": "ONLY in trades",
        "mtm": "ONLY in trades",
        "notional": "ONLY in trades",
        "failed_trade": "ONLY in trades",
        "adaptiv_code": "In BOTH tables (join key)",
        "as_of_date": "In BOTH tables",
        "currency": "In BOTH tables"
      }
    }
  },
  
  "tables": {
    "ccr_limits": {
      "description": "CCR exposure vs limits by counterparty",
      "columns": [
        "adaptiv_code", "customer_name", "sector", "rating", "country", "region",
        "portfolio", "booking_location", "currency", "as_of_date",
        "exposure_epe", "exposure_pfe", "exposure_ead", "exposure_var", "exposure_stress",
        "limit_ccr", "limit_type", "limit_buffer", "limit_utilization_pct", "risk_owner"
      ],
      "source": {
        "_comment_parquet_mode": "When mode='parquet', provide file_path to .parquet file (fast, handles 100M+ rows)",
        "file_path": "input/data/ccr_limits.parquet",
        
        "_comment_api_mode": "When mode='api', provide these fields instead",
        "_api_example": {
          "url": "https://api.example.com/ccr/limits",
          "headers": {
            "Authorization": "Bearer ${CCR_API_TOKEN}"
          },
          "format": "csv_rows_in_json",
          "_format_options": ["array_of_objects", "csv_rows_in_json", "csv_url"],
          "columns_key": "columns",
          "row_key": "rows",
          "field_key": null,
          "delimiter": ","
        }
      }
    },
    "trades": {
      "description": "Trade-level data for customers with limits. NOTE: failed_trade=true indicates exposure calculation failed; exposure is conservatively set equal to notional (overstated fallback).",
      "columns": [
        "trade_id", "adaptiv_code", "product", "notional", "mtm", "asset_class",
        "desk", "book", "trade_date", "maturity_date", "counterparty", "netting_set",
        "csa_flag", "collateralized", "risk_factor", "delta", "gamma", "vega", "pnl", "currency",
        "failed_trade", "as_of_date"
      ],
      "source": {
        "file_path": "input/data/trades.parquet"
      }
    }
  },
  
  "vocabulary": {
    "_comment": "Map natural language terms to database columns",
    "adaptiv code": "adaptiv_code",
    "utilization": "limit_utilization_pct",
    "limit utilization": "limit_utilization_pct",
    "counterparty code": "adaptiv_code",
    "breach": "limit_utilization_pct > 100",
    "headroom": "limit_ccr - exposure_pfe",
    "failed": "failed_trade = true",
    "failed exposure": "failed_trade = true (exposure calculation failed, set to notional)",
    "overstated exposure": "failed_trade = true (conservative fallback)",
    "latest": "as_of_date = (SELECT MAX(as_of_date) FROM ccr_limits)",
    "recent": "CAST(as_of_date AS DATE) >= CURRENT_DATE - INTERVAL 7 DAYS"
  },
  
  "limits": {
    "_comment": "SQL execution guardrails",
    "default_limit": 200,
    "hard_max_rows": 1000,
    "allow_non_select": false,
    "timeout_seconds": 30
  },
  
  "prompts": {
    "_comment": "LLM prompt templates (edit with caution)",
    "dialect_hint": "DuckDB SQL",
    "system": "You are a senior data analyst who writes safe, efficient SQL for {dialect_hint}. Use ONLY the provided tables/columns. Use qualified names. Only JOIN tables when columns from BOTH tables are needed. Return a single runnable SQL statement. No commentary.\n\nENTITY RELATIONSHIPS:\n- ccr_limits (1) ←→ (N) trades via adaptiv_code\n- ccr_limits: Counterparty-level aggregates (exposure, limits)\n- trades: Trade-level details (products, MTM, greeks)\n\nCOLUMN OWNERSHIP (CRITICAL):\n- limit_utilization_pct, exposure_*, limit_*: ONLY in ccr_limits\n- trade_id, product, mtm, notional, failed_trade: ONLY in trades  \n- adaptiv_code, as_of_date, currency: In BOTH tables\n\nDATA TYPES:\n- as_of_date is VARCHAR: ALWAYS use CAST(as_of_date AS DATE) for comparisons\n\nBUSINESS RULES:\n- failed_trade=true means exposure calculation failed; exposure is conservatively set equal to notional (overstated).",
    "user_template": "SCHEMA:\n{schema_text}\n\nVOCABULARY (synonyms → canonical):\n{vocabulary}\n\nTASK:\nWrite a single {dialect_hint} SQL statement for this request:\n\"{user_request}\"\n\nCRITICAL CONSTRAINTS:\n- limit_utilization_pct is ONLY in ccr_limits, NOT in trades\n- exposure_* columns are ONLY in ccr_limits\n- mtm, notional, product, failed_trade are ONLY in trades\n- as_of_date is VARCHAR: use CAST(as_of_date AS DATE) for date comparisons\n\nJOIN RULES (IMPORTANT):\n- If ALL columns you need are in ONE table, do NOT join - query that single table\n- Only JOIN when you need columns from BOTH ccr_limits AND trades\n- When joining: ccr_limits.adaptiv_code = trades.adaptiv_code\n\nRULES:\n- Use ONLY tables/columns in SCHEMA\n- Use vocabulary mappings when available\n- If sampling implied, apply LIMIT {default_limit}\n- SELECT/CTEs only (no writes)\n- Return SQL ONLY, no explanations"
  },
  
  "model": {
    "_comment": "LLM configuration (requires OPENAI_API_KEY in .env)",
    "provider": "openai",
    "name": "gpt-4o-mini",
    "_model_options": "Reasoning models: o1-mini (recommended), o1, o1-preview. Standard models: gpt-4o-mini, gpt-4o, gpt-4-turbo",
    "_note": "o1 models use extended reasoning, ignore temperature (fixed at 1.0), and auto-combine system+user prompts",
    "temperature": 0.0
  },
  
  "commentary": {
    "_comment": "Auto-generate natural language insights about query results (requires --use-llm or --with-commentary)",
    "enabled": true,
    "max_rows_in_prompt": 20,
    "system_prompt": "You are a data analyst. Provide factual insights using ONLY the data shown. No speculation. Each statement must cite specific numbers.\n\nBUSINESS CONTEXT:\n- failed_trade=true: Exposure calculation failed, conservatively set to notional (overstated).",
    "user_template": "USER ASKED: \"{user_request}\"\n\nDATA ({row_count} rows, showing first {sample_size}):\nColumns: {columns}\n{data_preview}\n\nProvide 2-4 concise bullet points that DIRECTLY ANSWER the question. Each bullet: cite specific numbers and explain what they mean in context. No generic statements."
  },
  
  "optimization": {
    "_comment": "Performance optimizations for large datasets (75M+ rows)",
    "two_stage_optimizer": true,
    "_explanation": "Two-stage: Extract WHERE filters first (Stage 1), then generate optimized SQL (Stage 2). Reduces LLM costs 30-50%, improves query performance with predicate pushdown."
  },
  
  "reference_values": {
    "_comment": "Provide LLM with actual unique values from data for better entity disambiguation",
    "enabled": true,
    "max_values_per_column": 20,
    "_explanation": "At runtime, extracts DISTINCT values from specified columns and injects into system prompt. Helps LLM distinguish between customer names vs product names, etc.",
    "_example": "customer_name: 'Aurora Metals', 'Northbridge Capital' vs product: 'FX FWD', 'IRS'",
    "tables": {
      "ccr_limits": {
        "columns": ["customer_name", "portfolio", "booking_location", "as_of_date"]
      },
      "trades": {
        "columns": ["product", "counterparty", "asset_class", "desk", "as_of_date"]
      }
    }
  },
  
  "agent": {
    "_comment": "LangGraph multi-turn agent with self-correction (use json_sql_agent.py --agent)",
    "enabled": false,
    "max_turns": 5,
    "system_prompt": "You are an expert SQL analyst. Generate accurate SQL for the user's question. Return ONLY the SQL query, no explanations or markdown. If refining based on feedback, return ONLY the corrected SQL query.\n\nKEY RULES:\n- Use qualified names. Only JOIN tables when columns from BOTH tables are needed\n- If ALL columns needed are in ONE table, do NOT join - query that single table\n- Column ownership: limit_utilization_pct/exposure_*/limit_* ONLY in ccr_limits; trade_id/product/mtm/notional/failed_trade ONLY in trades\n- ALWAYS include filter columns in SELECT so results are verifiable (e.g., if WHERE failed_trade=true, SELECT failed_trade)\n- Use LIKE patterns for text filtering (e.g., product LIKE '%FWD%' instead of product = 'fwd')",
    "exit_node": {
      "system_prompt": "You are a SQL quality evaluator. Determine if the query results adequately answer the user's question. Be critical and check for missing columns or incomplete filters.",
      "user_template": "USER ASKED: \"{user_request}\"\n\nSQL EXECUTED:\n{sql}\n\nRESULTS ({row_count} rows, showing first 5):\n{data_preview}\n\nEvaluate critically:\n1. Does the SQL have ALL filters mentioned in the user's request?\n2. Are filter columns (from WHERE clause) included in SELECT for verification?\n3. Do the results actually answer the question?\n4. Are there any logic errors (wrong columns, missing conditions, etc.)?\n5. If 0 results and using exact text match (= 'value'), should it use LIKE '%value%' instead?\n\nRespond with ONE of:\n- 'GOOD' if results adequately answer the question AND SQL is complete\n- 'REFINE: <specific issue>' if query needs correction (e.g., use LIKE instead of =, missing filter, missing column in SELECT, wrong table, unnecessary join)\n- 'EMPTY: <reason>' if zero results seem legitimately correct (no data exists)"
    },
    "refinement_template": "PREVIOUS ATTEMPT:\nSQL: {previous_sql}\nResults: {row_count} rows\n\nISSUE IDENTIFIED:\n{feedback}\n\nRefine the SQL to fix this issue. Remember:\n- Schema: {schema_hint}\n- Column ownership: limit_utilization_pct is ONLY in ccr_limits, mtm/notional/failed_trade are ONLY in trades\n- JOIN ONLY if you need columns from BOTH tables; otherwise query the single table directly\n- ALWAYS include filter columns in SELECT (e.g., if WHERE product LIKE '%FWD%', include product in SELECT)\n- Apply ALL user-specified filters (don't drop conditions from previous attempts)",
    "fallback_message": "Max refinement turns ({max_turns}) reached. Returning best attempt."
  },
  
  "rule_based_queries": {
    "_comment": "Pre-built queries that work without LLM",
    "top breaches headroom": "SELECT l.adaptiv_code, l.customer_name, l.limit_utilization_pct, (l.limit_ccr - l.exposure_pfe) AS headroom FROM ccr_limits l WHERE l.limit_utilization_pct > 100 ORDER BY l.limit_utilization_pct DESC"
  }
}

